In this project, a 4-layer LSTM model is used for English-to-Marathi translation with an encoder-decoder architecture. The encoder processes the input English sentence and generates a sequence of hidden states. The Bahdanau attention mechanism focuses on relevant encoder states dynamically at each decoding step, while the Luong attention mechanism computes a weighted sum of encoder states based on the decoder's hidden state. These attention mechanisms help improve translation accuracy by allowing the decoder to focus on specific parts of the input sequence, ensuring better alignment between source and target languages.
